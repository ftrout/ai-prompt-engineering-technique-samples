{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Prompt Engineer (APE) Example\n",
    "\n",
    "This notebook demonstrates **Automatic Prompt Engineer (APE)**, a meta-learning technique where the AI writes its own prompts.\n",
    "\n",
    "**The Concept:**\n",
    "Writing the \"perfect\" prompt is hard. APE automates this by:\n",
    "1.  **Generation**: The AI writes multiple different system prompts for a specific task.\n",
    "2.  **Evaluation**: It tests each prompt against a small set of \"Ground Truth\" examples (Input + Correct Answer).\n",
    "3.  **Selection**: It calculates an accuracy score for each prompt and selects the best one.\n",
    "\n",
    "**Key Mechanics:**\n",
    "* **The Architect**: A model that designs prompts.\n",
    "* **The Judge**: A scoring loop that runs candidates against test cases.\n",
    "* **The Result**: A scientifically optimized prompt that outperforms manual guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Authorization\n",
    "\n",
    "We start by importing the necessary libraries and loading your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport os\nimport random\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    api_key = input(\"Paste your OpenAI API key: \").strip()\n\nclient = OpenAI(api_key=api_key)\nprint(\"OpenAI client ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Architect: Generating Candidate Prompts\n",
    "\n",
    "In this step, we ask the model (the \"Architect\") to brainstorm 5â€“8 distinct, high-quality system prompts for a task you define. We use a high temperature to ensure diversity in the wording and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ape_generate_prompts(task, num_prompts=5):\n    \"\"\"Generates diverse system prompts for a specific task.\"\"\"\n    print(f\"Architect is designing {num_prompts} prompts for: '{task}'...\")\n    \n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[{\n                \"role\": \"user\", \n                \"content\": f\"\"\"\nGenerate {num_prompts} diverse, creative, high-quality system prompts that would make an LLM excel at this task:\n\nTask: {task}\n\nReturn ONLY a numbered list of the prompts. Do not include explanations.\n\"\"\"\n            }],\n            temperature=1.0  # High creativity to get different angles\n        )\n        \n        # Parse the output into a clean list\n        raw_text = response.choices[0].message.content\n        prompts = [\n            line.strip()[line.strip().find(\" \")+1:] \n            for line in raw_text.split(\"\\n\") \n            if line.strip() and any(c.isdigit() for c in line)\n        ]\n        return prompts\n        \n    except Exception as e:\n        print(f\"Error generating prompts: {e}\")\n        return []"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Judge: Evaluating Performance\n",
    "\n",
    "This function is the \"Test Harness.\" It takes a candidate prompt and runs it against your provided examples.\n",
    "\n",
    "* If the model's output matches your expected output, the prompt gets a point.\n",
    "* The final score is the percentage of correct answers (0.0 to 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ape_evaluate(prompts, test_examples):\n    \"\"\"\n    Tests each prompt against a list of (Input, Expected_Output) tuples.\n    Returns a sorted list of (Prompt, Accuracy_Score).\n    \"\"\"\n    print(\"\\nStarting evaluation tournament...\")\n    scores = []\n    \n    for i, prompt in enumerate(prompts):\n        correct_count = 0\n        print(f\"  Testing Prompt #{i+1}...\", end=\"\\r\")\n        \n        for input_text, expected_output in test_examples:\n            try:\n                # Run the candidate prompt\n                response = client.chat.completions.create(\n                    model=\"gpt-4o-mini\",\n                    messages=[\n                        {\"role\": \"system\", \"content\": prompt},\n                        {\"role\": \"user\", \"content\": input_text}\n                    ],\n                    temperature=0  # Zero temp for consistent testing\n                )\n                prediction = response.choices[0].message.content.strip()\n                \n                # Simple exact match or \"contains\" check\n                if expected_output.lower() in prediction.lower():\n                    correct_count += 1\n                    \n            except Exception as e:\n                print(f\"\\n  Error testing prompt {i+1}: {e}\")\n                continue\n        \n        accuracy = correct_count / len(test_examples) if test_examples else 0\n        scores.append((prompt, accuracy))\n        \n    print(f\"  Evaluation complete!              \")\n    # Sort by highest score\n    return sorted(scores, key=lambda x: x[1], reverse=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Optimization Loop\n",
    "\n",
    "Now we put it all together.\n",
    "\n",
    "1.  **Define Task**: You describe what you want (e.g., \"Classify movie reviews\").\n",
    "2.  **Provide Data**: You give 3-4 examples of correct behavior.\n",
    "3.  **Run APE**: The system writes prompts, tests them, and hands you the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Task\n",
    "task_description = input(\"Enter the task (e.g., 'Classify text as Spam or Not Spam'): \")\n",
    "\n",
    "# 2. Collect Ground Truth Data\n",
    "examples = []\n",
    "print(\"\\nNow provide 3-5 examples for testing.\")\n",
    "print(\"Type 'done' when finished.\\n\")\n",
    "\n",
    "while True:\n",
    "    inp = input(f\"Example {len(examples)+1} Input: \")\n",
    "    if inp.lower() == \"done\": break\n",
    "    out = input(f\"Example {len(examples)+1} Expected Output: \")\n",
    "    examples.append((inp, out))\n",
    "\n",
    "if not examples:\n",
    "    print(\"No examples provided. Using dummy data for demo...\")\n",
    "    task_description = \"Classify sentiment as Positive or Negative\"\n",
    "    examples = [\n",
    "        (\"I love this!\", \"Positive\"),\n",
    "        (\"This is terrible.\", \"Negative\"),\n",
    "        (\"Best day ever.\", \"Positive\"),\n",
    "        (\"I want a refund.\", \"Negative\")\n",
    "    ]\n",
    "\n",
    "# 3. Generate Candidates\n",
    "candidates = ape_generate_prompts(task_description)\n",
    "\n",
    "# 4. Evaluate & Rank\n",
    "ranked_prompts = ape_evaluate(candidates, examples)\n",
    "\n",
    "# 5. Show Results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ðŸ† WINNING PROMPT (Accuracy: {ranked_prompts[0][1]:.0%})\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\\"{ranked_prompts[0][0]}\\\"\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nRunner Up:\")\n",
    "if len(ranked_prompts) > 1:\n",
    "    print(f\"\\\"{ranked_prompts[1][0]}\\\" (Accuracy: {ranked_prompts[1][1]:.0%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}