{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought Prompting Example\n",
    "\n",
    "This notebook demonstrates **Chain of Thought (CoT)**, one of the most fundamental and effective prompt engineering techniques.\n",
    "\n",
    "**The Concept:**\n",
    "Large Language Models (LLMs) often struggle with complex math, logic, or reasoning tasks if forced to give an immediate answer. **Chain of Thought** encourages the model to \"show its work\" by breaking the problem down into intermediate steps before arriving at the final solution.\n",
    "\n",
    "**Key Mechanics:**\n",
    "1.  **Instruction**: We explicitly tell the model to plan and execute steps.\n",
    "2.  **Process**: The output will contain the reasoning path (The \"Thought\"), not just the result.\n",
    "3.  **Result**: Accuracy on complex tasks increases significantly because the model \"reasons\" through the logic vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Authorization\n",
    "\n",
    "We start by importing the `openai` library and securely loading your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport os\nimport time\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    api_key = input(\"Paste your OpenAI API key: \").strip()\n\n# Model configuration - can be overridden via environment variable\nMODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n\nclient = OpenAI(api_key=api_key)\nprint(f\"OpenAI client ready! Using model: {MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The System Prompt (The \"Instructions\")\n",
    "\n",
    "This is where we implement the technique. A standard prompt might just say \"You are a helpful assistant.\"\n",
    "\n",
    "For **Chain of Thought**, we modify the system prompt to enforce a structured thinking process:\n",
    "1.  **Understand**: Grasp the core request.\n",
    "2.  **Plan**: Outline the steps needed.\n",
    "3.  **Execute**: Perform the calculation or logic for each step.\n",
    "4.  **Conclude**: Provide the final answer only after the reasoning is complete.\n",
    "\n",
    "This significantly reduces \"hallucinations\" and calculation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that solves problems using chain of thought reasoning.\n",
    "\n",
    "For any query, do not just give the answer. Instead, break it down step by step:\n",
    "1. Understand the problem: Restate what needs to be solved.\n",
    "2. Plan the steps: Briefly list how you will approach it.\n",
    "3. Execute: Go through each step with detailed explanations.\n",
    "4. Final Answer: State the conclusion clearly at the end.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Interactive CoT Solver\n",
    "\n",
    "In this loop, we send your complex problem to the model.\n",
    "\n",
    "**Temperature Note**:\n",
    "We use a low temperature (`0.2`) here.\n",
    "* **Creative writing** benefits from high temperature (0.7–1.0).\n",
    "* **Logical reasoning/Math** requires low temperature (0.0–0.3) to ensure the model chooses the most probable (logical) next token at every step of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the conversation with the CoT instructions\nmessages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n\n# Maximum number of messages to keep (prevents token limit issues)\nMAX_HISTORY = 20\n\ndef trim_message_history(messages, max_messages=MAX_HISTORY):\n    \"\"\"Keep system prompt and last N-1 messages to prevent token overflow.\"\"\"\n    if len(messages) > max_messages:\n        return [messages[0]] + messages[-(max_messages - 1):]\n    return messages\n\nprint(\"Chain of Thought Solver Ready! Enter a problem (e.g., 'If I have 3 apples...'). Type 'quit' to exit.\\n\")\n\nwhile True:\n    # 1. Get User Input\n    user_input = input(\"Your problem: \")\n    \n    if user_input.strip().lower() in [\"quit\", \"exit\", \"stop\"]:\n        print(\"Goodbye!\")\n        break\n    \n    if not user_input.strip():\n        print(\"Please enter a valid problem.\")\n        continue\n    \n    # 2. Add to History\n    messages.append({\"role\": \"user\", \"content\": user_input})\n    \n    # 3. Generate Reasoning\n    try:\n        # We use a low temperature to make the logic deterministic and focused.\n        response = client.chat.completions.create(\n            model=MODEL,\n            messages=messages,\n            temperature=0.2 \n        )\n        \n        reply = response.choices[0].message.content\n        \n        # 4. Display the \"Chain\"\n        print(f\"\\nReasoning Process:\\n{reply}\\n\")\n        print(\"-\" * 60)\n        \n        # 5. Update History (Context)\n        # This allows you to ask follow-up questions about the logic provided.\n        messages.append({\"role\": \"assistant\", \"content\": reply})\n        \n        # 6. Trim history to prevent token overflow\n        messages = trim_message_history(messages)\n        \n        # 7. Rate limiting to avoid API throttling\n        time.sleep(0.2)\n        \n    except Exception as e:\n        print(f\"\\nError: {e}\\n\")\n        messages.pop()  # Remove the failed user message from history"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}