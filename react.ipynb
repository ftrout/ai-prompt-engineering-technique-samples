{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Prompting Example\n",
    "\n",
    "This notebook demonstrates **ReAct (Reason + Act)**, a technique that combines \"Chain of Thought\" reasoning with \"Action\" execution.\n",
    "\n",
    "**The Concept:**\n",
    "Humans solve problems by interleaving thinking and acting. We think \"I need to check the weather,\" we look it up (Act), we see it's raining (Observe), and then we think \"I should take an umbrella\" (Reason).\n",
    "ReAct forces the LLM to follow this exact cycle: **Thought → Action → Observation → Thought...**\n",
    "\n",
    "**Key Mechanics:**\n",
    "1.  **Thought**: The model analyzes the current situation.\n",
    "2.  **Action**: It decides on a specific tool or step to take next (e.g., \"Search Wikipedia\").\n",
    "3.  **Observation**: It looks at the result of that action (in this specific demo, we ask the model to *simulate* the observation to show the flow).\n",
    "4.  **Repeat**: This loop continues until the final answer is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Authorization\n",
    "\n",
    "We start by importing the necessary libraries and loading your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport os\nimport time\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    api_key = input(\"Paste your OpenAI API key: \").strip()\n\n# Model configuration - can be overridden via environment variable\nMODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n\nclient = OpenAI(api_key=api_key)\nprint(f\"OpenAI client ready! Using model: {MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The System Prompt (The \"Protocol\")\n",
    "\n",
    "We need to enforce a strict format. Unlike standard chat, a ReAct agent must output specific keywords (`Thought:`, `Action:`, `Observation:`) to structure its problem-solving.\n",
    "\n",
    "**Note on \"Simulation\":**\n",
    "In a production system, you would write code to parse the \"Action\" and actually execute a Python function or API call. For this prompt engineering demo, we instruct the model to **simulate** the observation so you can see the *reasoning trace* in action without needing external tools connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a smart agent using the ReAct (Reason + Act) framework.\n",
    "\n",
    "For every step, you must follow this strict format:\n",
    "1. Thought: Reason about the current state.\n",
    "2. Action: Describe the specific action or tool query you would use.\n",
    "3. Observation: Simulate a realistic result of that action.\n",
    "4. Repeat this cycle until you have the answer.\n",
    "5. Final Answer: State the conclusion clearly.\n",
    "\n",
    "Do not just guess. \"Use\" the tools (by simulating them) to get information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Interactive ReAct Agent\n",
    "\n",
    "This loop runs the agent.\n",
    "\n",
    "**Why use `temperature=0.3`?**\n",
    "* ReAct requires logic and strict adherence to a formatting protocol.\n",
    "* A lower temperature helps keep the model focused on the `Thought` -> `Action` structure without drifting into creative storytelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize history with the ReAct instructions\nmessages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n\n# Maximum number of messages to keep (prevents token limit issues)\nMAX_HISTORY = 20\n\ndef trim_message_history(messages, max_messages=MAX_HISTORY):\n    \"\"\"Keep system prompt and last N-1 messages to prevent token overflow.\"\"\"\n    if len(messages) > max_messages:\n        return [messages[0]] + messages[-(max_messages - 1):]\n    return messages\n\nprint(\"ReAct Agent Ready!\")\nprint(\"Enter a task that requires multiple steps (e.g., 'What is the population of the capital of France?').\")\nprint(\"Type 'quit' to exit.\\n\")\n\nwhile True:\n    # 1. Get User Input\n    user_input = input(\"Your task: \")\n    \n    if user_input.strip().lower() in [\"quit\", \"exit\"]:\n        print(\"Goodbye!\")\n        break\n    \n    if not user_input.strip():\n        print(\"Please enter a valid task.\")\n        continue\n    \n    # 2. Add to History\n    messages.append({\"role\": \"user\", \"content\": user_input})\n    \n    try:\n        # 3. Generate the ReAct Trace\n        response = client.chat.completions.create(\n            model=MODEL,\n            messages=messages,\n            temperature=0.3  # Low temp for logical structure\n        )\n        \n        reply = response.choices[0].message.content\n        \n        # 4. Display the Process\n        print(f\"\\nReasoning Trace:\\n{reply}\\n\")\n        print(\"-\" * 60)\n        \n        # 5. Save Context\n        messages.append({\"role\": \"assistant\", \"content\": reply})\n        \n        # 6. Trim history to prevent token overflow\n        messages = trim_message_history(messages)\n        \n        # 7. Rate limiting to avoid API throttling\n        time.sleep(0.2)\n        \n    except Exception as e:\n        print(f\"\\nError: {e}\\n\")\n        messages.pop()  # Remove the failed user message"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}