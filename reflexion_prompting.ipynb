{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflexion Prompting Example\n",
    "\n",
    "This notebook demonstrates **Reflexion**, a powerful technique for self-correction and iterative improvement.\n",
    "\n",
    "**The Concept:**\n",
    "Humans rarely get complex problems right on the first try. We draft, review, catch errors, and revise. **Reflexion** gives the AI this same ability. Instead of accepting the first answer, the model:\n",
    "1.  **Attempts** to solve the problem.\n",
    "2.  **Evaluates** its own work (or receives feedback).\n",
    "3.  **Reflects** on *why* it might be wrong (e.g., \"I missed a constraint\").\n",
    "4.  **Retries** with this new insight.\n",
    "\n",
    "**Key Mechanics:**\n",
    "* **The Solver**: A model instance trying to answer the query.\n",
    "* **The Reflector**: A model instance (often the same one) critiquing the previous attempt.\n",
    "* **The Loop**: A cycle of *Attempt â†’ Critique â†’ Reflect â†’ Retry* that continues until a valid answer is found or a limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Authorization\n",
    "\n",
    "We start by importing the necessary libraries and loading your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport os\nimport time\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    api_key = input(\"Paste your OpenAI API key: \").strip()\n\nclient = OpenAI(api_key=api_key)\nprint(\"OpenAI client ready! Using gpt-4o-mini (excellent at self-correction)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Reflexion Engine\n",
    "\n",
    "This function encapsulates the entire self-correction loop.\n",
    "\n",
    "**How it works:**\n",
    "1.  **History**: We keep a list of `reflections` (lessons learned from past failures).\n",
    "2.  **Prompting**: In every attempt, we inject the *previous reflections* into the prompt. This forces the model to specifically avoid repeating its past mistakes.\n",
    "3.  **Self-Check**: If the model doesn't output a specific success tag (`<FINAL_ANSWER>`), we assume it's still reasoning or unsure, and we trigger a specific \"Reflection Prompt\" to analyze what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def reflexion_solve(problem: str, max_reflections: int = 3):\n    reflections = []\n    attempts = []\n\n    print(f\"Problem: {problem}\\n\")\n    print(\"Starting Reflexion Agent...\\n\")\n    time.sleep(1)\n\n    for attempt in range(1, max_reflections + 2):\n        print(f\"--- ATTEMPT {attempt} ---\")\n        \n        # 1. Construct the Prompt with History\n        # If we have past reflections, we add them to the context.\n        reflection_context = \"\"\n        if reflections:\n            reflection_context = (\n                \"Previous reflections on mistakes:\\n\" + \n                \"\\n\".join(reflections[-3:]) + \n                \"\\n\\nUse these insights to avoid repeating errors.\\n\"\n            )\n\n        prompt = f\"\"\"\n{reflection_context}\n\nSolve this problem step by step:\n{problem}\n\nThink carefully.\nIf you are confident you have the correct solution, end your response with:\n<FINAL_ANSWER> [your answer here] </FINAL_ANSWER>\n\"\"\"\n\n        try:\n            # 2. Generate the Attempt\n            response = client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a reasoning agent. Learn from past mistakes to improve.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.7 if attempt == 1 else 0.5  # Lower temp on retries for focus\n            )\n            \n            reply = response.choices[0].message.content\n            print(f\"Model Output:\\n{reply[:200]}...\\n(truncated for brevity)\\n\")\n            attempts.append(reply)\n\n            # 3. Check for Success\n            if \"<FINAL_ANSWER>\" in reply:\n                answer = reply.split(\"<FINAL_ANSWER>\")[1].split(\"</FINAL_ANSWER>\")[0].strip()\n                print(f\"\\nâœ… SUCCESS! Final Answer: {answer}\\n\")\n                return answer, attempts, reflections\n\n            # 4. Trigger Reflection (if failed)\n            if attempt <= max_reflections:\n                print(\"\\nâš ï¸ No valid final answer detected. Triggering self-reflection...\")\n                \n                reflect_prompt = f\"\"\"\nThe previous attempt failed to produce a confident final answer.\n\nHere was the attempt:\n{reply}\n\nReflect deeply:\n- What went wrong?\n- What assumptions were incorrect?\n- What step was missed?\n- How should you reason differently next time?\n\nBe concise and honest.\n\"\"\"\n                reflect_response = client.chat.completions.create(\n                    model=\"gpt-4o-mini\",\n                    messages=[{\"role\": \"user\", \"content\": reflect_prompt}],\n                    temperature=0.7\n                )\n\n                reflection = reflect_response.choices[0].message.content.strip()\n                reflections.append(f\"Reflection {len(reflections)+1}: {reflection}\")\n                print(f\"ðŸ¤” Reflection: {reflection}\\n\")\n                time.sleep(1)\n                \n        except Exception as e:\n            print(f\"Error during attempt {attempt}: {e}\")\n            continue\n\n    print(\"âŒ Max reflections reached without a final answer.\")\n    return None, attempts, reflections"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing with Hard Problems\n",
    "\n",
    "Reflexion shines on problems that are \"tricky\"â€”where an impulsive answer is often wrong.\n",
    "\n",
    "**Examples provided:**\n",
    "1.  **Logic Puzzle**: The \"Three Switch\" problem (requires physical reasoning).\n",
    "2.  **Math Riddle**: The \"Lily Pad\" problem (requires exponential reasoning).\n",
    "3.  **Constraint Satisfaction**: Einstein's Riddle (requires keeping track of many variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined tricky problems\n",
    "problems = {\n",
    "    \"1\": \"\"\"\n",
    "You have 3 light bulbs in a room and 3 switches outside. Each switch controls one bulb.\n",
    "One switch turns it on/off, one does nothing, one turns it on only when the door is closed.\n",
    "You can enter the room only once.\n",
    "How do you determine which switch does what?\n",
    "\"\"\",\n",
    "    \"2\": \"\"\"\n",
    "A lily pad doubles in size every day. On day 30, it covers the entire pond.\n",
    "On what day did it cover half the pond?\n",
    "\"\"\",\n",
    "    \"3\": \"\"\"\n",
    "Solve this riddle:\n",
    "I speak without a mouth and hear without ears. I have no body, but I come alive with wind. What am I?\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Choose a problem to test Reflexion:\")\n",
    "print(\"1. Light Bulb Puzzle\")\n",
    "print(\"2. Lily Pad Math\")\n",
    "print(\"3. What Am I?\")\n",
    "print(\"4. Custom Input\")\n",
    "\n",
    "choice = input(\"\\nEnter 1-4: \").strip()\n",
    "\n",
    "if choice == \"4\":\n",
    "    selected_problem = input(\"Enter your problem: \")\n",
    "else:\n",
    "    selected_problem = problems.get(choice, problems[\"1\"])\n",
    "\n",
    "# Run the Agent\n",
    "final_ans, _, _ = reflexion_solve(selected_problem, max_reflections=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}