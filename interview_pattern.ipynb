{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Pattern Example\n",
    "\n",
    "This notebook demonstrates the **Interview Pattern**, a powerful prompt engineering technique where the AI controls the flow of information.\n",
    "\n",
    "**The Concept:**\n",
    "Instead of the user dumping all information at once, the AI acts as an interviewer. It asks specific questions one by one to gather the necessary context before providing a final recommendation.\n",
    "\n",
    "**Key Mechanics:**\n",
    "1.  **Persona**: The AI adopts a specific role (e.g., Career Coach).\n",
    "2.  **Iterative Loop**: It asks one question, waits for an answer, and repeats.\n",
    "3.  **State Management**: Since the OpenAI API is \"stateless\" (it doesn't remember past messages), we manually manage the conversation history.\n",
    "4.  **Stop Sequence**: We define a specific tag (`<FINAL_ANSWER>`) that the AI uses to signal it has enough information to finish the interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Authorization\n",
    "\n",
    "First, we import the necessary libraries and load your OpenAI API key. This setup ensures we can communicate with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\nimport time\n\nload_dotenv()\n\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif not api_key:\n    api_key = input(\"Paste your OpenAI API key: \").strip()\n\n# Model configuration - can be overridden via environment variable\nMODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n\nclient = OpenAI(api_key=api_key)\nprint(f\"OpenAI client ready! Using model: {MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The System Prompt (The \"Brain\")\n",
    "\n",
    "The **System Prompt** is the most critical part. It defines the rules of the \"game.\"\n",
    "\n",
    "We explicitly instruct the model to:\n",
    "* Ask **only one** question at a time.\n",
    "* Wait for the user's response.\n",
    "* **Never** give advice prematurely.\n",
    "* Use a specific **tag** (`<FINAL_ANSWER>`) when it's ready to conclude.\n",
    "\n",
    "This prevents the model from rambling or trying to solve the problem with insufficient data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert career coach helping a candidate improve their resume and prepare for interviews.\n",
    "\n",
    "You must use the Interview Pattern technique strictly:\n",
    "1. Ask ONLY ONE clear question at a time.\n",
    "2. Wait for the user's answer before asking the next.\n",
    "3. Remember everything said so far.\n",
    "4. When you have enough information to give excellent personalized advice, respond with exactly:\n",
    "   <FINAL_ANSWER>\n",
    "   followed by your full recommendation.\n",
    "5. Never give advice early.\n",
    "\n",
    "Start by greeting the user and asking your first question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initializing the Conversation\n",
    "\n",
    "Because the OpenAI API is **stateless**, we must keep a running list of all messages (the `messages` list).\n",
    "\n",
    "**The \"Hidden Trigger\" Trick:**\n",
    "The model usually waits for the user to speak first. To make the AI take the lead (greet us and ask the first question), we insert a hidden \"trigger\" message from the user side. We add this to the history but don't show it in the output, creating the illusion that the AI started the conversation on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize history with the System Prompt\nmessages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n\n# Maximum number of messages to keep (prevents token limit issues)\nMAX_HISTORY = 20\n\ndef trim_message_history(messages, max_messages=MAX_HISTORY):\n    \"\"\"Keep system prompt and last N-1 messages to prevent token overflow.\"\"\"\n    if len(messages) > max_messages:\n        return [messages[0]] + messages[-(max_messages - 1):]\n    return messages\n\nprint(\"Career Coach Interview Started! Type 'quit' to stop.\\n\")\n\n# Inject a hidden user message to kickstart the AI's persona\ninitial_trigger = \"Hello, please begin the career coaching interview.\"\nmessages.append({\"role\": \"user\", \"content\": initial_trigger})\n\ntry:\n    # Get the Coach's first response (Greeting + Question 1)\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=messages,\n        temperature=0.7\n    )\n    reply = response.choices[0].message.content\n\n    # Save the AI's reply to history\n    messages.append({\"role\": \"assistant\", \"content\": reply})\n\n    print(f\"Coach: {reply}\\n\")\n    \nexcept Exception as e:\n    print(f\"Error starting interview: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Interview Loop\n",
    "\n",
    "This is the main interactive engine.\n",
    "\n",
    "**What happens here:**\n",
    "1.  **Input:** We accept your answer to the coach's question.\n",
    "2.  **History Update:** We append your answer to the `messages` list.\n",
    "3.  **Generation:** We send the *entire* history back to OpenAI to get the next question.\n",
    "4.  **Stop Check:** We check the AI's response for the `<FINAL_ANSWER>` tag.\n",
    "    * If found: We strip the tag, print the final advice, and break the loop.\n",
    "    * If not found: The interview continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "while True:\n    # 1. Get User Input\n    user_input = input(\"You: \")\n    \n    if user_input.strip().lower() in [\"quit\", \"exit\", \"bye\", \"stop\"]:\n        print(\"\\nGood luck with your job hunt!\")\n        break\n    \n    if not user_input.strip():\n        print(\"Please enter a response.\")\n        continue\n    \n    # 2. Append User Input to History\n    messages.append({\"role\": \"user\", \"content\": user_input})\n    \n    try:\n        # 3. Generate Next Response\n        response = client.chat.completions.create(\n            model=MODEL,\n            messages=messages,\n            temperature=0.7\n        )\n        reply = response.choices[0].message.content\n        \n        # 4. Append AI Response to History\n        messages.append({\"role\": \"assistant\", \"content\": reply})\n        \n        # 5. Trim history to prevent token overflow\n        messages = trim_message_history(messages)\n        \n        # 6. Check for Conclusion\n        if \"<FINAL_ANSWER>\" in reply:\n            print(\"\\n\" + \"=\"*70)\n            print(\"FINAL RECOMMENDATION\")\n            print(\"=\"*70)\n            # Clean up the output by removing the control tag\n            print(reply.replace(\"<FINAL_ANSWER>\", \"\").strip())\n            print(\"=\"*70)\n            break\n        else:\n            # Continue the interview\n            print(f\"\\nCoach: {reply}\\n\")\n        \n        time.sleep(0.3)\n        \n    except Exception as e:\n        print(f\"\\nError: {e}\\n\")\n        messages.pop()  # Remove the failed user message"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}